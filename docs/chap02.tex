\chapter{经典多维尺度变换}
\label{cha:classMDS}


\section{介绍}
在本小节，我们介绍一下经典多维尺度变换的基本思想。最直接的，经典多维尺度变换的想法是最小化原始距离矩阵产生的Gram矩阵和重构后的坐标产生的新的Gram矩阵之间的差。故经典多维尺度变换的想法本质上类似于PCA算法。在PCA中，“信息”用原始变量之间的总方差来表示。PCA变化之后，希望尽可能的保持变量之间的这一“信息”,即若$\mathbf{X}$表示所有的变量，$\mathbf{X}$中的每一行，记为$\mathbf{x}_i$，表示每一个单个的变量。则PCA想尽可能的保持$\sum_{i=1}^nVar(\mathbf{x}_i) = trace(\Sigma_{\mathbf{X}})$。记$\Sigma_{\mathbf{X}}$的特征值分解为$\Sigma_{\mathbf{X}} = \mathbf{U}\Lambda\mathbf{U}^\intercal$, 且$\mathbf{U}^\intercal\mathbf{U} = \mathbf{I}$。故$trace(\Sigma_{\mathbf{X}}) = trace(\Lambda) = \sum_{i=1}^n\lambda_i$，其中$\lambda_i, (i=1, \dots, n)$为$\Sigma_{\mathbf{X}}$的特征值。

由此启发，用经典多维尺度变换解决$p$维空间中的重构问题的想法是，若已知的Gram矩阵记为$\mathbf{G}$，则希望找到一个矩阵$\mathbf{G}^* = \{\mathbf{G}^*_{ij}\}$，秩为$p$，能最小化$trace(\mathbf{G} - \mathbf{G}^*) = \sum_{i=1}^n(\lambda_i - \lambda_i^*)^2$。

\section{具体算法}
我们用$\mathbf{X} = [\mathbf{x}_1, \dots, \mathbf{x}_n]^\intercal$表示点集矩阵，其中$\mathbf{X}$的一行，$\mathbf{x}_i = (x_{i1}, \dots, x_{ip})^\intercal\in\mathbb{R}^p$表示一个点的坐标，且规定$\bar{\mathbf{x}}_i = 0$。
则定义距离平方矩阵$\mathbf{D} = \{d_{ij}^2\}\in\mathbb{R}^p$，其中$d_{ij}^2 = (\mathbf{x}_i - \mathbf{x}_j)^\intercal(\mathbf{x}_i - \mathbf{x}_j)$。定义centering matrix $\mathbf{J} = \mathbf{I} - \mathbf{1}\cdot\mathbf{1}^\intercal$，其中$\mathbf{I}$为单位矩阵，$\mathbf{1}$为元素全为1的列向量。同时定义Gram matrix或称inner product matrix为$\mathbf{G = XX^\intercal}$
%%定理证明

\begin{theorem}
假设$\mathbf{J}$和$\mathbf{G}$，$\mathbf{D}$如上述定义，则有$\mathbf{G = -\frac{1}{2} JDJ}$
\end{theorem}
\begin{proof}
由定义$\mathbf{G}_{ij} = \sum_{k=1}^p x_{ik}x_{jk} = \mathbf{x}_i^\intercal \mathbf{x}_j$，$d_{ij}^2 = (\mathbf{x}_i - \mathbf{x}_j)^\intercal (\mathbf{x}_i - \mathbf{x}_j) = \mathbf{G}_{ii} + \mathbf{G}_{jj} - 2\mathbf{G}_{ij}$. 

由$\overline{\mathbf{x}}_i = 0$，则$\sum_{i=1}^n\mathbf{G}_{ij} = \sum_{j=1}^n\mathbf{G}_{ij} = 0$.

\begin{equation}
    \frac{1}{n}\sum_{i=1}^nd_{ij}^2 = \frac{1}{n}\sum_{i=1}^n\mathbf{G}_{ii} + \mathbf{G}_{jj}
\end{equation}

\begin{equation}
    \frac{1}{n}\sum_{j=1}^nd_{ij}^2 = \frac{1}{n}\sum_{j=1}^n\mathbf{G}_{jj} + \mathbf{G}_{ii}
\end{equation}

\begin{equation}
    \frac{1}{n^2}\sum_{i=1}^n\sum_{j=1}^nd_{ij}^2 = \frac{2}{n}\sum_{i=1}^n\mathbf{G}_{ii}
\end{equation}

由上述(1) (2) (3)知：
\begin{equation*}
    \mathbf{G}_{jj} = \frac{1}{n}\sum_{i=1}^nd_{ij}^2 - \frac{1}{n}\sum_{i=1}^n\mathbf{G}_{ii}
\end{equation*}

\begin{equation*}
    \mathbf{G}_{ii} = \frac{1}{n}\sum_{j=1}^nd_{ij}^2 - \frac{1}{n}\sum_{j=1}^n\mathbf{G}_{jj}
\end{equation*}

\begin{equation*}
    \frac{2}{n}\sum_{i=1}^n\mathbf{G}_{ii} = \frac{1}{n^2}\sum_{i=1}^n\sum_{j=1}^nd_{ij}^2
\end{equation*}

故带入
\begin{equation*}
    \mathbf{G}_{ij} = \frac{1}{2}(\mathbf{G}_{ii} + \mathbf{G}_{jj} - d_{ij}^2)
\end{equation*}

中得
\begin{equation*}
    \mathbf{G}_{ij} = -\frac{1}{2}(d_{ij}^2 - d_{i\cdot}^2 - d_{\cdot j}^2 + d_{\cdot\cdot}^2)
\end{equation*}
\end{proof}

%%%%第二个定理
\begin{theorem}
若$rank(\mathbf{X}) = p$，则$rank(\mathbf{G}) = rank(\mathbf{XX^\intercal}) = rank(\mathbf{X}) = p$
\end{theorem}
\begin{proof}
即要证明$\forall A\in \mathbb{R}^{n\times n}, rank(A^\intercal A) = rank(A)$。

则证明$\forall X\in \mathbb{R}^n$，$AX = 0$和$A^\intercal A = 0$同解。

首先$AX = 0$的解显然也是$A^\intercal AX = 0$的解。

另一方面由$A^\intercal AX = 0$可以推知$X^\intercal A^\intercal AX = 0$，即$(AX)^\intercal AX = 0$。故可推知$AX = 0$

故由上可知$rank(A) = rank(A^\intercal A)$，同理可知$rank(A^\intercal) = rank(AA^\intercal)$。故原等式得证。
\end{proof}

由上述两定理可知，当已知距离平方矩阵并确定重构空间维数$p$之后，我们可以得到一个秩为$p$的inner product矩阵$G$，由于矩阵$G$为对称半正定矩阵，则它应该由$p$个非零特征值和$(n-p)$个零特征值。记矩阵$G$的特征值分解为$G = U\Lambda U^\intercal$, $U_1 = (u_1, \dots, u_p)$, $\Lambda_1 = diag(\lambda_1, \dots, \lambda_p)$。则$\hat{\mathbf{X}} = U_1\Lambda_1^{\frac{1}{2}}$为重构的$p$维空间中点的坐标。具体算法见Algorithm \ref{alg: classicalMDS}。
\begin{algorithm}
\caption{经典多维尺度变换算法} 
\label{alg: classicalMDS}
\begin{algorithmic}[1]
\REQUIRE{距离矩阵$\mathbf{D}$，希望重构的空间维数$p$，点的个数$n$}
\STATE{由Theorem 1，计算$G = -\frac{1}{2}\mathbf{JDJ}$}
\STATE{计算矩阵$\mathbf{G}$的特征值分解，$G = \mathbf{U}\Lambda \mathbf{U}^\intercal$，其中$\Lambda = diag(\lambda_1,\dots,\lambda_n), \mathbf{U} = (u_1, \dots, u_n)$}
\STATE{记$\Lambda_1 = diag(\lambda_1,\dots,\lambda_p)$为$\mathbf{G}$最大的$p$个特征值，$\mathbf{U}_1 = (u_1,\dots,u_p)$为其相对应的特征向量}
\STATE{\begin{equation*}
    \mathbf{G}^* = \mathbf{U}_1\Lambda_1\mathbf{U}_1^\intercal
     = (\mathbf{U_1}\Lambda_1^{\frac{1}{2}})(\Lambda_1^{\frac{1}{2}}\mathbf{U_1})^\intercal = \mathbf{XX}^\intercal
\end{equation*}
其中$\mathbf{X} = \mathbf{U}_1\Lambda_1^{\frac{1}{2}}$}
\RETURN{$\mathbf{X}$}
\end{algorithmic}
\end{algorithm}

最后说明一下最开始假设$\bar{\mathbf{x}}_i = 0$的合理性。由$\mathbf{J\cdot1} = 0$，故$\mathbf{G\cdot1} = 0$，则
\begin{equation*}
    n^2\bar{\mathbf{X}}\bar{\mathbf{X}}^\intercal = (\bar{\mathbf{X}}^\intercal\mathbf{1})^\intercal(\bar{\mathbf{X}}^\intercal\mathbf{1}) = \mathbf{1}^\intercal\mathbf{G1} = 0
\end{equation*}
故$\bar{\mathbf{X}} = 0$。
容易发现，当距离平方矩阵完整或者缺失值较少的时候，传统MDS方法恢复的坐标效果是不错的，但是当缺失值过大之后，这种类似于主成分分析的方法是不能很好的运作的，在下一部分我们介绍一种新的MDS方法来弥补这一点。